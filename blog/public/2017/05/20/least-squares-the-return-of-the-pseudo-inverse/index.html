<!DOCTYPE html>
<html lang="en-us">
<head>
    <title>Least squares: the return of the Pseudo-inverse &middot; </title>
    <meta name="generator" content="Hugo 0.27.1" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="author" content="Math vs. Machine">
    
      <meta name="description" content="">
    
    
     <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.4.4/css/bulma.css" />
    <link rel="canonical" href="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/"/>
    <link rel="icon" href="https://mathvsmachine.com/favicon.ico">
    <link rel="apple-touch-icon" href="https://mathvsmachine.com/apple-touch-icon.png"/>
    <link rel="stylesheet" href="https://mathvsmachine.com/css/style.css">
    <link rel="stylesheet" href="https://mathvsmachine.com/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://mathvsmachine.com/css/monokai.css">
    <link rel="stylesheet" href="https://mathvsmachine.com/fancybox/jquery.fancybox.css">
    
    <link rel="stylesheet" href="https://mathvsmachine.com/css/theorems.css">
    
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,400,600' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Source+Code+Pro' rel='stylesheet' type='text/css'>
    <meta property="og:title" content="Least squares: the return of the Pseudo-inverse" />
<meta property="og:description" content="In this post we will discuss the so-called least squares problem (a horribly outdated name), which asks to find the best approximation to a certain vector by using a given linear map.
The problem is usually set up as follows: We let $V$ and $W$ denote finite-dimensional inner product spaces and we&rsquo;re given a linear map $f \in \textrm{Hom}(V,W)$, together with a vector $w \in W$. The challenge is now to describe the vectors $w^&#43; \in V$ such that $f(w^&#43;)$ is as close to $w$ as possible." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/" />



<meta property="article:published_time" content="2017-05-20T11:58:06&#43;02:00"/>
<meta property="article:modified_time" content="2017-05-20T11:58:06&#43;02:00"/>






  
  







    
    
<meta itemprop="name" content="Least squares: the return of the Pseudo-inverse">
<meta itemprop="description" content="In this post we will discuss the so-called least squares problem (a horribly outdated name), which asks to find the best approximation to a certain vector by using a given linear map.
The problem is usually set up as follows: We let $V$ and $W$ denote finite-dimensional inner product spaces and we&rsquo;re given a linear map $f \in \textrm{Hom}(V,W)$, together with a vector $w \in W$. The challenge is now to describe the vectors $w^&#43; \in V$ such that $f(w^&#43;)$ is as close to $w$ as possible.">


<meta itemprop="dateModified" content="2017-05-20T11:58:06&#43;02:00" />
<meta itemprop="wordCount" content="830">



<meta itemprop="keywords" content="linear regression,linear algebra," />

    <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Least squares: the return of the Pseudo-inverse"/>
<meta name="twitter:description" content="In this post we will discuss the so-called least squares problem (a horribly outdated name), which asks to find the best approximation to a certain vector by using a given linear map.
The problem is usually set up as follows: We let $V$ and $W$ denote finite-dimensional inner product spaces and we&rsquo;re given a linear map $f \in \textrm{Hom}(V,W)$, together with a vector $w \in W$. The challenge is now to describe the vectors $w^&#43; \in V$ such that $f(w^&#43;)$ is as close to $w$ as possible."/>

    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
</head>
<body>




	<header id="header">
  <div id="header-main" class="header-inner">
    <div class="outer">
      <a href="https://mathvsmachine.com" id="logo">
          
          <span class="site-title"></span>
      </a>
      <nav id="main-nav">
          
          
          <a class="main-nav-link" href="https://mathvsmachine.com/">Home</a>
          
          
          
          
          
          
          
          

          
          <a class="main-nav-link" href="/about/">About</a>
          

          
          
          
          
          <a class="main-nav-link" href="https://mathvsmachine.com/tags/">Tags</a>
          
          
          
          <a class="main-nav-link" href="https://mathvsmachine.com/files/book.pdf">The Book</a>
          
          
          
          <a class="main-nav-link" href="https://mathvsmachine.com/series/">Post Series</a>
          
          
      </nav>
        <nav id="sub-nav">
          <div class="profile" id="profile-nav">
            <a id="profile-anchor" href="javascript:;"><img class="avatar" src="https://mathvsmachine.com/css/images/bobble_logo-circle.png"><i class="fa fa-caret-down"></i></a>
          </div>
        </nav>
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form">
              <input type="search" name="q" class="search-form-input" placeholder="Search">
              <button type="submit" class="search-form-submit">
              </button>
              <input type="hidden" name="sitesearch" value="https://mathvsmachine.com" />
         </form>
        </div>
    </div>
  </div>
  <div id="main-nav-mobile" class="header-sub header-inner">
    <table class="menu outer">
      <tbody>
          <tr>
          
          
          <td><a class="main-nav-link" href="https://mathvsmachine.com/">Home</a></td>
          
          
          
          
          
          
          
          

          
          <td><a class="main-nav-link" href="/about/">About</a></td>
          

          
          
          
          
          <td><a class="main-nav-link" href="https://mathvsmachine.com/tags/">Tags</a></td>
          
          
          
          <td><a class="main-nav-link" href="https://mathvsmachine.com/files/book.pdf">The Book</a></td>
          
          
          
          <td><a class="main-nav-link" href="https://mathvsmachine.com/series/">Post Series</a></td>
          
          
          <td>
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form">
          <input type="search" name="q" class="search-form-input" placeholder="Search">
          <input type="hidden" name="sitesearch" value="https://mathvsmachine.com" />
          </form>
        </td>
      </tr>
      </tbody>
    </table>
  </div>
</header>

   	
   	<div class="outer">
   	
    	<aside id="profile">
  <div class="inner profile-inner">
    <div class="base-info profile-block">
      
      <img id="avatar" src="https://mathvsmachine.com/css/images/bobble_logo-circle.png">
      
      <h2 id="name">Math vs. Machine</h2>
      <h3 id="title">Louis de Thanhoffer de Volcsey</h3>
      
      
          <a id="follow" href="https://louisdethanhoffer.com">
              Website
          </a>
      
    </div>
    <div class="article-info profile-block">
      <div class="article-info-block">
        5
        <span>Posts</span>
      </div>
      <div class="article-info-block">
        
          3
        
        <span>
            Tags
        </span>
      </div>
    </div>
    <div class="profile-block social-links">
      <table>
        <tr>
          
<td><a href="//github.com/ldethanhoffer" target="_blank" title="GitHub"><i class="fa fa-github"></i></a></td>



<td><a href="//linkedin.com/in/louisdethanhoffer" target="_blank" title="LinkedIn"><i class="fa fa-linkedin"></i></a></td>




<td><a href="//quora.com/Louis-de-Thanhoffer-de-Volcsey" target="_blank" title="Quora"><i class="fa fa-quora"></i></a></td>




<td><a href="//quora.com/Louis-de-Thanhoffer-de-Volcsey" target="_blank" title="Quora"><i class="fa fa-quora"></i></a></td>

          <td><a href="https://mathvsmachine.com/index.xml" target="_blank" title="RSS"><i class="fa fa-rss"></i></a></td>
        </tr>
      </table>
    </div>
  </div>
</aside>

    

    <section id="main">
    
    <article id="page-undefined" class="article article-type-page" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
            <img src="https://mathvsmachine.com/banners/normal_equation.png" class="article-banner">
        

        

<header class="article-header">
    <a href="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/">
    <h1 class="article-title" itemprop="name">
        Least squares: the return of the Pseudo-inverse
    </h1>
    </a>
    <div class="article-meta">

        
        <div class="article-date">
            <i class="fa fa-calendar"></i>
            <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
            &middot;
            
            830
            words
            &middot;
            
            
            
        
        </div>
        
        
            
            
            <div class="article-category">
                <i class="fa fa-folder"></i>
                
                
                <a class="article-category-link" href="https://mathvsmachine.com/series/linear-regression">linear regression</a>
                
                
            </div>
            
        
        
        
            
            
            <div class="article-category">
                <i class="fa fa-list-alt"></i>
                
                
                <a class="article-category-link" href="https://mathvsmachine.com/series/linear-regression">linear regression</a>
                
                
            </div>
            
        

    </div>
</header>

        <div class="article-entry" itemprop="articleBody">
            <p>In this post we will discuss the so-called <em>least squares</em> problem (a horribly outdated name), which asks to find the best approximation to a certain vector by using a given linear map.</p>

<p>The problem is usually set up as follows: We let $V$ and $W$ denote finite-dimensional inner product spaces and we&rsquo;re given a linear map $f \in \textrm{Hom}(V,W)$, together with a vector $w \in W$. The challenge is now to describe the vectors $w^+ \in V$ such that $f(w^+)$ is as close to $w$ as possible. In other words:
$$
\vert \vert w-f(w^+)\vert \vert =\min_{v \in V}\vert \vert w-f(v)\vert \vert
$$
We&rsquo;ll call such a vector $w^+$ a <em>least squares approximation of $w$ by the map $f$</em> (or LSA for short)<br />
Today, we&rsquo;ll show you how we can always find LSA&rsquo;s for any $f$ and $w$ and we&rsquo;ll tell you how to find them!<br />
To begin, let&rsquo;s recall a classical fact from linear algebra that reinterprets the above minimality condition a little more geometrically:
<div class="lemma">
Let $U\subset W$ be any subspace. Let $w\in W$ and $u\in U$. Then $$\vert \vert w-u\vert \vert =\min_{u&rsquo; \in U}\vert \vert w-u&rsquo;\vert \vert \iff \big(v-u\big)\perp U$$
</div>
<div class = 'proof'>
The interested reader can find a proof of this fact in The Book<br />
</div></p>

<p>We can give a slightly more useful interpretation of this result as follows: recall that for any subspace $U$, it&rsquo;s always possible to decompose the space $W$ into $U\oplus U^\perp =W$. In fact, if $\pi_W:W\longrightarrow U$ denotes the canonical projection, then this decomposition of $w$ into the components $U$ and $U^\perp$ is given by $w=\pi_U(w)+\big(w-\pi_U(w)\big)$. The above lemma nsimply tells us that $\pi_U(w)$ is the unique vector in $U$ that minimizes the distance $\vert \vert w-u\vert \vert$ among all choices of $u \in U$.<br />
Now, if we choose $U$ to be the subspace $\textrm{im}(f)\subset W$, then this tells us the following:</p>

<div class="lemma">
    Let $w\in W$, $f \in \textrm{Hom}(V,W)$ and $v \in V$. Then the following are equivalent:
    <ol>
        <li> $v$ is  a least squares approximation to $w$ by $f$ </li>
        <li> $f(v)=\pi_{\textrm{im}(f)}(w)$ where $\pi_{\textrm{im}(f)}:W\longrightarrow \textrm{im(f)}$ is the canonical projection under the decomposition $W=\textrm{im}(f)\oplus \textrm{im}(f)^\perp$</li>
    </ol>
    </div>
The above already proves that any vector $w$ indeed has a least squares approximation!<br><br>  
We can even do a little better and givee a first description of set of LSA's to $w$: indeed, if $w^+$ is a LSA, then $v$ is also an LSA if and only if $f(v)=\pi(w)=f(w^+)$, so that the set of LSA's is given by the affine subspace $w^+ + \ker(f)\subset V$, where $w^+$ is any specific choice of LSA.<br> <br>
In order to understand how to construct a particualr choice of LSA, let's analyze the condition $f(v)=\pi(w)$ in a little more detail...<br><br>
Since $w$ decomposes as $w=\pi(w)+\big(w-\pi(w)\big)=f(v)+\big(w-f(v)\big)$ in $\textrm{im}(f)\oplus \textrm{im}(f)^\perp$, we already know that $f(v)=\pi(w)\iff \big(f(v)-\pi(w)\big) \in \textrm{im}(f)^\perp$. We can simply rewrite this last condition as follows:
$$0=\langle w-f(v),f(v')\rangle =\langle f^{\ast} (w)-f^\ast( f(v)),v'\rangle \text{ for all } v' \in V $$
So that $f(v)=\pi(w)\iff f^\ast(w)=\big(f^\ast\circ f\big)(v)$.<br>
This condition is usually referred to as the normal equation:
<div class = 'lemma'>
    Let $w \in W$ and $f \in \textrm{Hom}(V,W)$. Then $v$ is a least squares approximation to $w$ by $f$ if and only if $v$ is a solution to the normal equation $$f^\ast(w)=\big(f^\ast\circ f\big)(v) $$
</div>
This gives a very nice reinterpretation of the least squares problem but it still doesn't give us an explicit construction of a specific LSA in terms of $f$ and $w$. It turns out that this is exactly what a Moore-Penrose pseudo-inverse does. We'll summarize our discussion of this concept in the following way:
<div class = 'lemma'>
    Let $\ker(f)\oplus U_V=V$ and $\textrm{im}(f)\oplus U_W=W$. Then
    <ol>
        <li>the restriction $f:U_V\longrightarrow \textrm{im}(f)$ has an inverse $f^\sharp:\textrm{im}(f)\longrightarrow U_V$</li>
        <li>This inverse has a unique lift $f^\sharp: W\longrightarrow U_V$ in the sense that $f^\sharp = \pi_{\textrm{im}(f)}\circ f^\sharp$, or equivalenty $$ f \circ f^\sharp = \pi_{\textrm{im}(f)}$$</li>
    </ol>
The Moore-Penrose inverse of $f$ is the map $f^+$ that corresponds to the choices $U_V=\ker(f)^\perp$ and  $U_W=\textrm{im}(f)^\perp$. In other words $f^+: W\longrightarrow \ker(f)^\perp$ is the unique map that satisfies $f \circ f^+ = \pi_{\textrm{im}(f)}$
</div>

<p>Our discussion shows that $f^+$ was indeed the exact missing ingredient: we&rsquo;ve been hunting for a vector $v$ that satisfies $f(v)=\pi_{\textrm{im}(f)}(w)$ all along and the above lemma shows that $v=f^+(w)$ does the job!<br><br> Putting everything together we now can describe the solution to the least squares problem as follows:</p>

<div class = 'theorem'>
Let $w \in W$ and $f \in \textrm{Hom}(V,W)$ and $v \in V$. The following are equivalent:
<ol>
    <li> $v$ is a least squares approximation of $w$ by $f$  </li>
    <li>  $v$ is a solution to the normal equation $f^\ast(w)=\big(f^\ast\circ f\big)(v)$ </li>
    <li> $v \in f^+(w)+\ker(f)$ where $f^+:W \longrightarrow V$ is the Moore-Penrose inverse of $f$ </li>
</ol>
</div>

<p>If $f$ is injective, we can give the above theorem a little more meat.<br><br> Indeed, in this case the endomorphism $f^* \circ f$ becomes invertible (the transpose of an injective map is also injective so that $f^\ast \circ f$ is an injective  hence invertible).<br> Using this, the normal equation can now be rewritten as $\big(f^\ast\circ f\big)^{-1} (f^\ast(w))=v$. So that:</p>

<div class ='corollary'>
if $f$ is injective. Then $w$ has a unique LSA given by 
$$
w^+ = \big(f^\ast\circ f\big)^{-1} \big(f^\ast(w)\big)
$$
</div>

        </div>
        <footer class="article-footer">
    <a data-url="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/" data-id="ed6fd3082a9be6f86260bdfdf4dac444" class="article-share-link">
        <i class="fa fa-share"></i>
        Share
    </a>
    
    <a href="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/#disqus_thread" class="article-comment-link">
        Comments
    </a>
    

    <script>
    (function ($) {
        
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fa fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fa fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fa fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fa fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
    </script>
</footer>

    </div>

    
<nav id="article-nav">
    
    <a href="https://mathvsmachine.com/2017/05/20/proving-linear-regression/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">
          Older
      </strong>
      <div class="article-nav-title">Proving Linear Regression</div>
    </a>
    

    
    <a href="https://mathvsmachine.com/2017/05/20/introducing-linear-regression..a-little-differently/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">
          Newer
      </strong>
      <div class="article-nav-title">Introducing linear regression..a little differently</div>
    </a>
    
</nav>


</article>


<section id="comments">
    <div id="disqus_thread">
        <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "louis-de-thanhoffer" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
</section>


    </section>

   	
    	<aside id="sidebar">
    



<div class="widget-wrap">
    <h3 class="widget-title">
        Recents
    </h3>
    <div class="widget">
        <ul id="recent-post">
            
            <li>
                <div class="item-thumbnail">
                    <a href="https://mathvsmachine.com/2017/05/20/defining-supervised-learning/" class="thumbnail">
                    
                        <span style="background-image:url(https://mathvsmachine.com/thumbnails/supervised_learning.png)" alt="Least squares: the return of the Pseudo-inverse" class="thumbnail-image"></span>
                    
                    </a>
                </div>
                <div class="item-inner">
                    
                    
                    <p class="item-title"><a href="https://mathvsmachine.com/2017/05/20/defining-supervised-learning/" class="title">Defining Supervised Learning</a></p>
                    <p class="item-date">
                        <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
                    </p>
                </div>
            </li>
            
            <li>
                <div class="item-thumbnail">
                    <a href="https://mathvsmachine.com/2017/05/20/dissecting-pseudo-inverses/" class="thumbnail">
                    
                        <span style="background-image:url(https://mathvsmachine.com/thumbnails/linear_regression.png)" alt="Least squares: the return of the Pseudo-inverse" class="thumbnail-image"></span>
                    
                    </a>
                </div>
                <div class="item-inner">
                    
                    
                    <p class="item-title"><a href="https://mathvsmachine.com/2017/05/20/dissecting-pseudo-inverses/" class="title">Dissecting Pseudo-inverses</a></p>
                    <p class="item-date">
                        <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
                    </p>
                </div>
            </li>
            
            <li>
                <div class="item-thumbnail">
                    <a href="https://mathvsmachine.com/2017/05/20/introducing-linear-regression..a-little-differently/" class="thumbnail">
                    
                        <span style="background-image:url(https://mathvsmachine.com/thumbnails/linear_regression.png)" alt="Least squares: the return of the Pseudo-inverse" class="thumbnail-image"></span>
                    
                    </a>
                </div>
                <div class="item-inner">
                    
                    
                    <p class="item-title"><a href="https://mathvsmachine.com/2017/05/20/introducing-linear-regression..a-little-differently/" class="title">Introducing linear regression..a little differently</a></p>
                    <p class="item-date">
                        <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
                    </p>
                </div>
            </li>
            
            <li>
                <div class="item-thumbnail">
                    <a href="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/" class="thumbnail">
                    
                        <span style="background-image:url(https://mathvsmachine.com/thumbnails/linear_regression.png)" alt="Least squares: the return of the Pseudo-inverse" class="thumbnail-image"></span>
                    
                    </a>
                </div>
                <div class="item-inner">
                    
                    
                    <p class="item-title"><a href="https://mathvsmachine.com/2017/05/20/least-squares-the-return-of-the-pseudo-inverse/" class="title">Least squares: the return of the Pseudo-inverse</a></p>
                    <p class="item-date">
                        <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
                    </p>
                </div>
            </li>
            
            <li>
                <div class="item-thumbnail">
                    <a href="https://mathvsmachine.com/2017/05/20/proving-linear-regression/" class="thumbnail">
                    
                        <span style="background-image:url(https://mathvsmachine.com/thumbnails/linear_algebra.png)" alt="Least squares: the return of the Pseudo-inverse" class="thumbnail-image"></span>
                    
                    </a>
                </div>
                <div class="item-inner">
                    
                    
                    <p class="item-title"><a href="https://mathvsmachine.com/2017/05/20/proving-linear-regression/" class="title">Proving Linear Regression</a></p>
                    <p class="item-date">
                        <time datetime="2017-05-20 11:58:06 &#43;0200 &#43;0200" itemprop="datePublished">2017-05-20</time>
                    </p>
                </div>
            </li>
            
        </ul>
    </div>
</div>


    



    



    





<div class="widget-wrap">
    <h3 class="widget-title">
        Tag cloud
    </h3>
    <div class="widget tagcloud">
        
        
        <a href="https://mathvsmachine.com/tags/linear-algebra" style="font-size: 12px;">linear-algebra</a>
        
        
        <a href="https://mathvsmachine.com/tags/linear-regression" style="font-size: 12px;">linear-regression</a>
        
        
        <a href="https://mathvsmachine.com/tags/supervised-learning" style="font-size: 12px;">supervised-learning</a>
        
    </div>
</div>




    




<div class="widget-wrap">
    <h3 class="widget-title"></h3>
    <div class="widget">
        <ul class="category-list">
            
            <li class="category-list-item">
                
                <a class="category-list-link" href="https://mathvsmachine.com/series/linear-regression">
                    linear-regression
                </a>
                <span class="category-list-count">4</span>
            </li>
            
            <li class="category-list-item">
                
                <a class="category-list-link" href="https://mathvsmachine.com/series/supervised-learning">
                    supervised-learning
                </a>
                <span class="category-list-count">1</span>
            </li>
            
        </ul>
    </div>
</div>





    <div id="toTop" class="fa fa-angle-up"></div>
</aside>

    
	


<footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018
      Powered by <a href="//gohugo.io">Hugo</a>.
    </div>
  </div>
</footer>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-120720172-1', 'auto');
ga('send', 'pageview');
</script>

<script src="https://mathvsmachine.com/fancybox/jquery.fancybox.pack.js"></script>
<script src="https://mathvsmachine.com/js/script.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>


<script src="https://sonoisa.github.io/xyjax_ext/xypic.js"></script>

<script>hljs.initHighlightingOnLoad();</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>




</body>
</html>